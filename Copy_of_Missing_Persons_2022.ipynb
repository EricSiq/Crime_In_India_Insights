{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricSiq/Crime_In_India_Insights/blob/main/Copy_of_Missing_Persons_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Problem Statement & Objective\n",
        "\n"
      ],
      "metadata": {
        "id": "103XP1mE1inD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project analyzes district-wise missing persons data in India for the year 2022. Using unsupervised learning techniques, we aim to uncover regional patterns, detect anomalies, and visualize clusters. The dataset includes demographic breakdowns by gender and age group across states and union territories.\n",
        "\n"
      ],
      "metadata": {
        "id": "2u9YyCEI12hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Importing Essential Libraries"
      ],
      "metadata": {
        "id": "Ck9Z6dR119Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.manifold import TSNE, MDS\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.patches as mpatches\n",
        "import umap\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "3aEI0kfo1-rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Load the Dataset"
      ],
      "metadata": {
        "id": "vACOrW9z2V0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the dataset\n",
        "file_path = '/content/DistrictwiseMissingPersons2022.csv'\n",
        "\n",
        "# Try loading the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(\"Dataset shape:\", df.shape)\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "6iP4fvWs2RWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Data Preprocessing"
      ],
      "metadata": {
        "id": "iRdBvKCI2hh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Region Mapping"
      ],
      "metadata": {
        "id": "Z4Y3bJvs2mTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Function to map states to regions\n",
        "def map_region(state):\n",
        "    south = [\"Andhra Pradesh\", \"Telangana\", \"Karnataka\", \"Tamil Nadu\", \"Kerala\", \"Puducherry\", \"Lakshadweep\", \"AN Islands\"]\n",
        "    west = [\"Maharashtra\", \"Goa\", \"Gujarat\", \"Daman and Diu\", \"DN Haveli and Daman Diu\"]\n",
        "    northeast = [\"Arunachal Pradesh\", \"Assam\", \"Manipur\", \"Meghalaya\", \"Mizoram\", \"Nagaland\", \"Tripura\", \"Sikkim\"]\n",
        "    north = [\"Kashmir\", \"Himachal Pradesh\", \"Punjab\", \"Uttarakhand\", \"Haryana\", \"Uttar Pradesh\", \"Rajasthan\", \"Bihar\",\n",
        "             \"Chhattisgarh\", \"West Bengal\", \"Odisha\", \"Chandigarh\", \"Delhi\", \"Ladakh\", \"Jharkhand\", \"Madhya Pradesh\"]\n",
        "\n",
        "    if state.strip() in south:\n",
        "        return \"South India\"\n",
        "    elif state.strip() in west:\n",
        "        return \"West Coast\"\n",
        "    elif state.strip() in northeast:\n",
        "        return \"North East\"\n",
        "    elif state.strip() in north:\n",
        "        return \"North India\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "# Apply region mapping to the dataset\n",
        "df['Region'] = df['State'].apply(map_region)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fubyd32w2hHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Filter Rows"
      ],
      "metadata": {
        "id": "kVebiR342xNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove leading/trailing spaces in district names\n",
        "df['District'] = df['District'].str.strip()\n",
        "\n",
        "# Split into two datasets: all districts and the summary row\n",
        "total_districts = df[df['District'] == \"Total Districts\"]\n",
        "all_districts = df[df['District'] != \"Total Districts\"]\n",
        "\n",
        "# Display shapes of the two datasets\n",
        "print(\"Total Districts shape:\", total_districts.shape)\n",
        "print(\"All Districts shape:\", all_districts.shape)\n"
      ],
      "metadata": {
        "id": "L96TCcxP2uxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Scaling the Data"
      ],
      "metadata": {
        "id": "qQNyl8vq24ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. EDA (Exploratory Data Analysis)."
      ],
      "metadata": {
        "id": "eNkxFBwA3Bej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1 Outlier Check Using Boxplots"
      ],
      "metadata": {
        "id": "QqaS4gtb3Kv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select numeric columns for scaling\n",
        "numeric_cols_total = total_districts.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create a copy and scale the data\n",
        "total_districts_scaled = total_districts.copy()\n",
        "total_districts_scaled[numeric_cols_total] = scaler.fit_transform(total_districts_scaled[numeric_cols_total])\n"
      ],
      "metadata": {
        "id": "qqyXd8mG3NoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.2 Distribution Plots for Scaled Features"
      ],
      "metadata": {
        "id": "9uwGGzmt3WAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Calculate layout\n",
        "n_cols = len(numeric_cols_total)\n",
        "n_rows = math.ceil(n_cols / 4)\n",
        "\n",
        "# Plot all distributions\n",
        "plt.figure(figsize=(16, n_rows * 4))\n",
        "for i, col in enumerate(numeric_cols_total):\n",
        "    plt.subplot(n_rows, 4, i + 1)\n",
        "    sns.histplot(total_districts_scaled[col], kde=True, bins=20, color='skyblue')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.suptitle(\"Distribution of Features (Total Districts)\", fontsize=18)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-UVg3oVv3YnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Splitting Data into Train and Test Sets"
      ],
      "metadata": {
        "id": "WI3YHIJQ3ioa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy"
      ],
      "metadata": {
        "id": "RnG9gwXWW69E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Improved region mapping with both geographic and demographic approaches\n",
        "def map_geographic_region(state):\n",
        "    \"\"\"Traditional geographic mapping\"\"\"\n",
        "    south = [\"Andhra Pradesh\", \"Telangana\", \"Karnataka\", \"Tamil Nadu\", \"Kerala\", \"Puducherry\", \"Lakshadweep\", \"AN Islands\"]\n",
        "    west = [\"Maharashtra\", \"Goa\", \"Gujarat\", \"Daman and Diu\", \"DN Haveli and Daman Diu\"]\n",
        "    northeast = [\"Arunachal Pradesh\", \"Assam\", \"Manipur\", \"Meghalaya\", \"Mizoram\", \"Nagaland\", \"Tripura\", \"Sikkim\"]\n",
        "    north = [\"Kashmir\", \"Himachal Pradesh\", \"Punjab\", \"Uttarakhand\", \"Haryana\", \"Uttar Pradesh\", \"Rajasthan\", \"Bihar\",\n",
        "             \"Chhattisgarh\", \"West Bengal\", \"Odisha\", \"Chandigarh\", \"Delhi\", \"Ladakh\", \"Jharkhand\", \"Madhya Pradesh\"]\n",
        "\n",
        "    state_clean = state.strip()\n",
        "    if state_clean in south:\n",
        "        return \"South_India\"\n",
        "    elif state_clean in west:\n",
        "        return \"West_Coast\"\n",
        "    elif state_clean in northeast:\n",
        "        return \"North_East\"\n",
        "    elif state_clean in north:\n",
        "        return \"North_India\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "# First apply the geographic mapping to the dataset\n",
        "df['Region'] = df['State'].apply(map_geographic_region)\n",
        "\n",
        "# Apply same mapping to total_districts dataframe\n",
        "total_districts['Region'] = total_districts['State'].apply(map_geographic_region)\n",
        "\n",
        "# 2. Feature engineering before scaling\n",
        "# Add domain-specific ratios to total_districts\n",
        "total_districts['Child_Adult_Ratio'] = (\n",
        "    (total_districts['Male_Children'] + total_districts['Female_Children']) /\n",
        "    (total_districts['Total_Male'] + total_districts['Total_Female'])\n",
        ")\n",
        "total_districts['Gender_Ratio'] = total_districts['Total_Male'] / total_districts['Total_Female'].replace(0, 1)\n",
        "total_districts['Child_Gender_Ratio'] = total_districts['Male_Children'] / total_districts['Female_Children'].replace(0, 1)\n",
        "total_districts['Missing_Density'] = total_districts['Total_Male'] + total_districts['Total_Female']\n",
        "\n",
        "# 3. Apply scaling with both standard and robust scalers\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "\n",
        "# Select numeric columns for scaling\n",
        "numeric_cols_total = total_districts.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create copies for each scaling method\n",
        "total_districts_standard = total_districts.copy()\n",
        "total_districts_robust = total_districts.copy()\n",
        "\n",
        "# Apply scalers\n",
        "scaler_standard = StandardScaler()\n",
        "scaler_robust = RobustScaler()\n",
        "\n",
        "total_districts_standard[numeric_cols_total] = scaler_standard.fit_transform(total_districts[numeric_cols_total])\n",
        "total_districts_robust[numeric_cols_total] = scaler_robust.fit_transform(total_districts[numeric_cols_total])\n",
        "\n",
        "# 4. Feature importance analysis\n",
        "# We'll use a simple correlation analysis for feature selection\n",
        "correlation_matrix = total_districts_standard[numeric_cols_total].apply(\n",
        "    lambda x: x.abs().corr(total_districts_standard['Gender_Ratio'].abs())\n",
        ")\n",
        "\n",
        "# Plot feature importances based on correlation\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': correlation_matrix.index,\n",
        "    'Correlation': correlation_matrix.values\n",
        "}).sort_values('Correlation', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Correlation', y='Feature', data=importance_df)\n",
        "plt.title('Feature Correlation with Gender Ratio')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Select top features based on correlation\n",
        "# Take features with correlation above the median\n",
        "top_features = importance_df[importance_df['Correlation'] > importance_df['Correlation'].median()]['Feature'].tolist()\n",
        "\n",
        "# Add our engineered features to the list if not already included\n",
        "engineered_features = ['Child_Adult_Ratio', 'Gender_Ratio', 'Child_Gender_Ratio', 'Missing_Density']\n",
        "for feature in engineered_features:\n",
        "    if feature in numeric_cols_total and feature not in top_features:\n",
        "        top_features.append(feature)\n",
        "\n",
        "print(f\"Selected {len(top_features)} features out of {len(numeric_cols_total)}\")\n",
        "\n",
        "# 6. Improved train/test split with stratification\n",
        "# We'll try both standard and robust scaling to see which works better\n",
        "X_standard = total_districts_standard[top_features]\n",
        "X_robust = total_districts_robust[top_features]\n",
        "y = total_districts['Region']\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Handling class imbalance issue\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Check if we have classes with very few samples\n",
        "min_samples = y.value_counts().min()\n",
        "if min_samples < 3:\n",
        "    print(f\"\\nWarning: Smallest class has only {min_samples} samples.\")\n",
        "    print(\"Using stratification in train/test split might fail.\")\n",
        "    print(\"Consider using a different region mapping or synthetic sampling.\")\n",
        "\n",
        "    # Determine if we need to use stratification\n",
        "    use_stratify = min_samples >= 2\n",
        "else:\n",
        "    use_stratify = True\n",
        "\n",
        "# Perform train/test split\n",
        "if use_stratify:\n",
        "    X_train_std, X_test_std, y_train, y_test = train_test_split(\n",
        "        X_standard, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    X_train_rob, X_test_rob, _, _ = train_test_split(\n",
        "        X_robust, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "else:\n",
        "    # If stratification not possible, do regular split\n",
        "    X_train_std, X_test_std, y_train, y_test = train_test_split(\n",
        "        X_standard, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    X_train_rob, X_test_rob, _, _ = train_test_split(\n",
        "        X_robust, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "# 7. Verify the train/test distribution\n",
        "print(\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nTest set class distribution:\")\n",
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "JYmPUIg2VI9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Remove Underrepresented Classes\n",
        "Classes with less than 3 samples cause issues in both training and evaluation."
      ],
      "metadata": {
        "id": "wcNipr2Y-Vj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Drop classes with <3 samples\n",
        "value_counts = y_train.value_counts()\n",
        "valid_classes = value_counts[value_counts >= 3].index\n",
        "\n",
        "X_train_filtered = X_train_std[y_train.isin(valid_classes)]\n",
        "y_train_filtered = y_train[y_train.isin(valid_classes)]\n",
        "X_test_filtered = X_test_std[y_test.isin(valid_classes)]\n",
        "y_test_filtered = y_test[y_test.isin(valid_classes)]\n",
        "\n",
        "# 2. Impute missing values using median\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "X_train_filtered = pd.DataFrame(imputer.fit_transform(X_train_filtered), columns=X_train_filtered.columns)\n",
        "X_test_filtered = pd.DataFrame(imputer.transform(X_test_filtered), columns=X_test_filtered.columns)\n",
        "\n",
        "# 3. Apply PCA without limiting components for explained variance plot\n",
        "pca_full = PCA()\n",
        "X_train_pca_full = pca_full.fit_transform(X_train_filtered)\n",
        "\n",
        "# 4. Plot cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca_full.explained_variance_ratio_), marker='o', linestyle='--', color='darkblue')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by PCA Components')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.9, color='red', linestyle='--', label='90% Variance\n"
      ],
      "metadata": {
        "id": "9DrRUmJb-QI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. PCA - Principal Component Analysis"
      ],
      "metadata": {
        "id": "xanM_ph53pdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Drop classes with <3 samples\n",
        "value_counts = y_train.value_counts()\n",
        "valid_classes = value_counts[value_counts >= 3].index\n",
        "\n",
        "X_train_filtered = X_train_std[y_train.isin(valid_classes)]\n",
        "y_train_filtered = y_train[y_train.isin(valid_classes)]\n",
        "X_test_filtered = X_test_std[y_test.isin(valid_classes)]\n",
        "y_test_filtered = y_test[y_test.isin(valid_classes)]\n",
        "\n",
        "# 2. Impute missing values using median\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "X_train_filtered = pd.DataFrame(imputer.fit_transform(X_train_filtered), columns=X_train_filtered.columns)\n",
        "X_test_filtered = pd.DataFrame(imputer.transform(X_test_filtered), columns=X_test_filtered.columns)\n",
        "\n",
        "# 3. Apply PCA without limiting components for explained variance plot\n",
        "pca_full = PCA()\n",
        "X_train_pca_full = pca_full.fit_transform(X_train_filtered)\n",
        "\n",
        "# 4. Plot cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca_full.explained_variance_ratio_), marker='o', linestyle='--', color='darkblue')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by PCA Components')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.9, color='red', linestyle='--', label='90% Variance')\n",
        "plt.axhline(y=0.95, color='green', linestyle='--', label='95% Variance')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 5. Apply PCA with selected number of components (e.g., 5)\n",
        "pca = PCA(n_components=5)\n",
        "X_train_pca = pca.fit_transform(X_train_filtered)\n",
        "X_test_pca = pca.transform(X_test_filtered)\n",
        "\n",
        "# 6. Check explained variance\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Total Explained Variance (5 components):\", pca.explained_variance_ratio_.sum())\n"
      ],
      "metadata": {
        "id": "ocVvU5wu3mBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  9. LDA – Linear Discriminant Analysis"
      ],
      "metadata": {
        "id": "2pzAmhaR4VMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LDA(n_components=1)  # Max components = number of classes - 1 (we have 4 regions → max = 3)\n",
        "X_train_lda = lda.fit_transform(X_train_filtered, y_train_filtered)\n",
        "X_test_lda = lda.transform(X_test_filtered)\n",
        "\n",
        "# Pad with zeros to make it 2D for plotting later\n",
        "X_train_lda = np.hstack([X_train_lda, np.zeros_like(X_train_lda)])\n",
        "X_test_lda = np.hstack([X_test_lda, np.zeros_like(X_test_lda)])\n"
      ],
      "metadata": {
        "id": "ST9yeBMl4GXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_full = LDA(n_components=None)\n",
        "lda_full.fit(X_train_filtered, y_train_filtered)\n",
        "print(\"LDA Explained Variance Ratio:\", lda_full.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "EUyUB8Ag4sSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. SVD"
      ],
      "metadata": {
        "id": "G1KifXZW5JNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying more components initially to observe explained variance\n",
        "svd_check = TruncatedSVD(n_components=10, random_state=42)\n",
        "svd_check.fit(X_train_filtered)\n",
        "\n",
        "explained_variance_svd = svd_check.explained_variance_ratio_.cumsum()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance_svd) + 1), explained_variance_svd, marker='o', linestyle='--', color='orange')\n",
        "plt.title('SVD - Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.90, color='red', linestyle='--', label='90% Threshold')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9HD3Pv5E5Mzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final SVD for visualization\n",
        "svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "X_train_svd = svd.fit_transform(X_train_filtered)\n",
        "X_test_svd = svd.transform(X_test_filtered)\n"
      ],
      "metadata": {
        "id": "lf4_Dnpe5ewS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. MDS"
      ],
      "metadata": {
        "id": "unkJYBoM5kRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mds = MDS(n_components=2, random_state=42, n_init=1, max_iter=300, dissimilarity='euclidean')\n",
        "X_train_mds = mds.fit_transform(X_train_filtered)\n",
        "X_test_mds = mds.fit_transform(X_test_filtered)\n",
        "\n",
        "# Reset y_train and y_test indices to match transformed arrays\n",
        "y_train_mds = y_train.reset_index(drop=True)\n",
        "y_test_mds = y_test.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "HN-9dlAx5f2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. T-SNE"
      ],
      "metadata": {
        "id": "yFy_iRB-5tYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set perplexity based on data size\n",
        "perplexity = min(5, X_train_filtered.shape[0] - 1)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, max_iter=1000)\n",
        "X_train_tsne = tsne.fit_transform(X_train_filtered)\n",
        "X_test_tsne = tsne.fit_transform(X_test_filtered)\n",
        "\n",
        "# Reset indices for matching\n",
        "y_train_tsne = y_train.reset_index(drop=True)\n",
        "y_test_tsne = y_test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "J4bpg_qI6XeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. UMap\n"
      ],
      "metadata": {
        "id": "8btCUlmU6Zap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
        "\n",
        "# Fit and transform\n",
        "X_train_umap = umap_model.fit_transform(X_train_filtered)\n",
        "X_test_umap = umap_model.transform(X_test_filtered)"
      ],
      "metadata": {
        "id": "dNeONBdd6hr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.1 Clustering on UMap"
      ],
      "metadata": {
        "id": "a-uoH_RU6zkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_train_umap)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    score = silhouette_score(X_train_umap, kmeans.labels_)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot Inertia (Elbow Method)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(K_range, inertia, 'o-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "\n",
        "# Plot Silhouette Score\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(K_range, silhouette_scores, 's-', color='green')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score vs. k')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "njJ1QK3F6rug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Perform KMeans clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "umap_clusters = kmeans.fit_predict(X_train_umap)\n",
        "\n",
        "# Add cluster labels to a DataFrame for visualization\n",
        "umap_cluster_df = pd.DataFrame(X_train_umap, columns=['UMAP1', 'UMAP2'])\n",
        "umap_cluster_df['Cluster'] = umap_clusters\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=umap_cluster_df, x='UMAP1', y='UMAP2', hue='Cluster', palette='Set2', s=60)\n",
        "plt.title(\"KMeans Clustering (k=4) on UMAP Output\")\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5_0gyEMr8vAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Comparative Visualization of Dimensionality Reduction Methods"
      ],
      "metadata": {
        "id": "YFodRQnf89Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create label-to-color mapping\n",
        "unique_labels = sorted(y_train_filtered.unique())\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "colors = sns.color_palette(\"tab10\", len(unique_labels))\n",
        "color_map = {label: colors[idx] for label, idx in label_mapping.items()}\n",
        "legend_handles = [mpatches.Patch(color=color_map[label], label=label) for label in unique_labels]\n",
        "\n",
        "# Data to plot\n",
        "method_names = [\"PCA\", \"SVD\", \"t-SNE\", \"MDS\", \"UMAP\"]\n",
        "method_data = [X_train_pca, X_train_svd, X_train_tsne, X_train_mds, X_train_umap]\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 10), constrained_layout=True)\n",
        "fig.suptitle(\"2D Visualization of Dimensionality Reduction Methods\", fontsize=20, fontweight=\"bold\", color=\"darkred\")\n",
        "\n",
        "for ax, data, name in zip(axes.flat, method_data, method_names):\n",
        "    for label in unique_labels:\n",
        "        idxs = y_train == label\n",
        "        ax.scatter(\n",
        "            data[idxs, 0], data[idxs, 1],\n",
        "            color=color_map[label], label=label, edgecolors='white', linewidth=0.5, s=40, alpha=0.9\n",
        "        )\n",
        "    ax.set_title(name, fontsize=16, fontweight=\"bold\")\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    ax.set_xlabel(\"Component 1\")\n",
        "    ax.set_ylabel(\"Component 2\")\n",
        "\n",
        "# Turn off extra subplot if any\n",
        "if len(method_data) < len(axes.flat):\n",
        "    axes.flat[-1].axis('off')\n",
        "\n",
        "# Add legend\n",
        "fig.legend(handles=legend_handles, loc='lower right', bbox_to_anchor=(0.93, 0.93), fontsize=12, title=\"Regions\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "3iXWUvia9APN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15.Model Evaluation"
      ],
      "metadata": {
        "id": "f8xaYGyZ9N4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tune_knn(X_train_red, y_train_red, X_test_red, y_test_red, method=\"KNN\"):\n",
        "    param_grid = {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'p': [1, 2]  # Manhattan and Euclidean distances\n",
        "    }\n",
        "\n",
        "    model = KNeighborsClassifier()\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train_red, y_train_red)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test_red)\n",
        "\n",
        "    acc = accuracy_score(y_test_red, y_pred)\n",
        "    class_report = classification_report(y_test_red, y_pred, zero_division=1)\n",
        "    conf_matrix = confusion_matrix(y_test_red, y_pred)\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    print(f\"{method} Tuned Accuracy: {acc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"classification_report\": class_report,\n",
        "        \"confusion_matrix\": conf_matrix,\n",
        "        \"best_params\": best_params\n",
        "    }\n"
      ],
      "metadata": {
        "id": "JNZe0a9s9M79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_knn = {}\n",
        "results_knn['PCA'] = tune_knn(X_train_pca, y_train_filtered, X_test_pca, y_test_filtered, method=\"PCA + KNN\")\n",
        "results_knn['LDA'] = tune_knn(X_train_lda, y_train_filtered, X_test_lda, y_test_filtered, method=\"LDA + KNN\")\n",
        "results_knn['SVD'] = tune_knn(X_train_svd, y_train_filtered, X_test_svd, y_test_filtered, method=\"SVD + KNN\")\n",
        "results_knn['MDS'] = tune_knn(X_train_mds, y_train_mds, X_test_mds, y_test_mds, method=\"MDS + KNN\")\n",
        "results_knn['t-SNE'] = tune_knn(X_train_tsne, y_train_tsne, X_test_tsne, y_test_tsne, method=\"t-SNE + KNN\")\n"
      ],
      "metadata": {
        "id": "PXgavtpn9aMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = {method: results_knn[\"accuracy\"] for method, results_knn in results_knn.items()}\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(accuracies.keys(), accuracies.values(), color='skyblue')\n",
        "plt.xlabel(\"Dimensionality Reduction Method\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QWD7gszl9xxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"Best Params: {result['best_params']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "3VcYdW7h9y8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Classification Report:\\n{result['classification_report']}\\n\")\n"
      ],
      "metadata": {
        "id": "8XK4R2KW_UFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Confusion Matrix:\\n{result['confusion_matrix']}\\n\")\n"
      ],
      "metadata": {
        "id": "GKxEV6Wy_X1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Confusion Matrix:\\n{result['confusion_matrix']}\\n\")\n"
      ],
      "metadata": {
        "id": "0yHblFF9_cHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}