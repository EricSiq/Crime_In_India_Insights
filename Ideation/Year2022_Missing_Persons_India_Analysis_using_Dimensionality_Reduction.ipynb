{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricSiq/India_Missing_Persons_Analysis_2017-2022/blob/main/2022_Missing_Persons_India_Analysis_using_Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised Machine Learning Lab Project:\n",
        "\n",
        "    Dimensionality Reduction and Comparision of Algorithms\n",
        "\n",
        "By:\n",
        "\n",
        "    Eric Siqueira (23070126041)\n",
        "    Dipti Kothari (23070126040)\n",
        "\n",
        "AIML A2\n",
        "\n",
        "Sem IV"
      ],
      "metadata": {
        "id": "6UsJxS-RN_Mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Problem Statement & Objective\n",
        "\n"
      ],
      "metadata": {
        "id": "103XP1mE1inD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    5 Years (2017-2022) Districtwise Indian Missing Persons Dataset Available at:\n",
        "https://www.kaggle.com/datasets/ericsiq/india-5-years-districtwise-missing-persons-dataset\n",
        "\n",
        "    GitHub Repo:\n",
        "\n",
        "https://github.com/EricSiq/India_Missing_Persons_Analysis_2017-2022"
      ],
      "metadata": {
        "id": "pzIkPDK8Ngz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project analyzes district-wise missing persons data in India for the year 2022. Using unsupervised learning techniques, we aim to uncover regional patterns, detect anomalies, and visualize clusters. The dataset includes demographic breakdowns by gender and age group across states and union territories.\n",
        "\n"
      ],
      "metadata": {
        "id": "2u9YyCEI12hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Importing Essential Libraries"
      ],
      "metadata": {
        "id": "Ck9Z6dR119Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.manifold import TSNE, MDS\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.patches as mpatches\n",
        "import umap\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import math\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n"
      ],
      "metadata": {
        "id": "3aEI0kfo1-rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Load Dataset"
      ],
      "metadata": {
        "id": "vACOrW9z2V0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section imports the dataset from a specified CSV file path (`DistrictwiseMissingPersons2022.csv`) into a pandas DataFrame. It checks whether the file is successfully loaded by printing the shape (rows and columns) and displaying the first few rows.\n",
        "\n",
        "If the file is not found at the specified location, it raises an appropriate error message.\n",
        "\n"
      ],
      "metadata": {
        "id": "bHcsEkyMS3O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/DistrictwiseMissingPersons2022.csv'\n",
        "\n",
        "# Check loading of the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(\"Dataset shape:\", df.shape)\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n"
      ],
      "metadata": {
        "id": "6iP4fvWs2RWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop or impute missing values\n",
        "df.dropna(inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "GMC2Yv3sjC_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Data Pre-processing"
      ],
      "metadata": {
        "id": "iRdBvKCI2hh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Region Mapping"
      ],
      "metadata": {
        "id": "Z4Y3bJvs2mTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, I split the dataset into 4 geographical sections so as to better understand the local hotspots of Missing persons in India. The regions include: North India, South India, Western Coast and Northeastern 7 Sister States; based on the 'State' column value.\n",
        "\n",
        "I defined a function *'map_region'* that categorizes Indian states and UTs into broader regions. It then calls this function to the 'State' column of the dataset and creates a new column 'Region' that stores the corresponding region label for each row.\n",
        "\n",
        "> This regional grouping is useful for later visual analysis & EDA based on geographical zones.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b4xYlLqCRirT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to map states to regions\n",
        "def map_region(state):\n",
        "    south = [\"Andhra Pradesh\", \"Telangana\", \"Karnataka\", \"Tamil Nadu\", \"Kerala\", \"Puducherry\", \"Lakshadweep\", \"AN Islands\"]\n",
        "    west = [\"Maharashtra\", \"Goa\", \"Gujarat\", \"Daman and Diu\", \"DN Haveli and Daman Diu\"]\n",
        "    northeast = [\"Arunachal Pradesh\", \"Assam\", \"Manipur\", \"Meghalaya\", \"Mizoram\", \"Nagaland\", \"Tripura\", \"Sikkim\"]\n",
        "    north = [\"Kashmir\", \"Himachal Pradesh\", \"Punjab\", \"Uttarakhand\", \"Haryana\", \"Uttar Pradesh\", \"Rajasthan\", \"Bihar\",\n",
        "             \"Chhattisgarh\", \"West Bengal\", \"Odisha\", \"Chandigarh\", \"Delhi\", \"Ladakh\", \"Jharkhand\", \"Madhya Pradesh\"]\n",
        "\n",
        "    if state.strip() in south:\n",
        "        return \"South India\"\n",
        "    elif state.strip() in west:\n",
        "        return \"West Coast\"\n",
        "    elif state.strip() in northeast:\n",
        "        return \"North East\"\n",
        "    elif state.strip() in north:\n",
        "        return \"North India\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "# Apply region mapping to the dataset\n",
        "df['Region'] = df['State'].apply(map_region)\n"
      ],
      "metadata": {
        "id": "fubyd32w2hHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Filter Rows"
      ],
      "metadata": {
        "id": "kVebiR342xNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section refines and separates the dataset for targeted analysis:\n",
        "\n",
        "*    It first removes any leading or trailing whitespace from entries in the 'District' column to avoid inconsistencies due to formatting errors.\n",
        "\n",
        "*    It then splits the dataset into two parts:\n",
        "\n",
        "\n",
        "  1.   total_districts: rows where the 'District' column equals \"Total Districts\" (aggregate data of each state in the dataset).\n",
        "  2.   all_districts: all other rows representing individual districts.\n",
        "\n",
        "*   Finally, it prints the shape (rows, columns) of both datasets to verify the separation. This step is to ensure accurate analysis, especially when aggregate rows could skew results."
      ],
      "metadata": {
        "id": "hX73dqr_W0TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove leading/trailing spaces in district names\n",
        "df['District'] = df['District'].str.strip()\n",
        "\n",
        "# Split into two datasets: all districts and the summary row\n",
        "total_districts = df[df['District'] == \"Total Districts\"]\n",
        "all_districts = df[df['District'] != \"Total Districts\"]\n",
        "\n",
        "# Display shapes of the two datasets\n",
        "print(\"Total Districts shape:\", total_districts.shape)\n",
        "print(\"All Districts shape:\", all_districts.shape)\n"
      ],
      "metadata": {
        "id": "L96TCcxP2uxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "1y0GnY4La4lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*for the EDA, I have focused on these 5 parameters to highlight the inequalities that exist in the missing peoples data across India:*\n",
        "\n",
        "\n",
        "\n",
        "1.   Gender Distribution Analysis:\n",
        "\n",
        " By summing the total counts of missing males, females, and transgender individuals, the analysis provides a comprehensive overview of gender disparities among missing persons. Visualizing this data through bar plots facilitates immediate recognition of gender-based trends.​\n",
        "\n",
        "2.   Age Grouping and Comparative Analysis:\n",
        "\n",
        "Categorizing data into specific age brackets allows for the examination of age-related vulnerabilities. Stacked bar charts illustrate the intersection of age and gender, highlighting groups that may require targeted interventions.​\n",
        "\n",
        "3.   Regional Distribution Examination:\n",
        "\n",
        "Aggregating data by regions and visualizing the gender distribution within each region identifies geographic areas with higher incidences of missing persons, aiding in regional policy formulation.​\n",
        "\n",
        "4.   Correlation Analysis:\n",
        "\n",
        "Age Categories vs. Total Missing Cases: Computing and visualizing the correlation matrix between different age categories and the grand total of missing cases reveals potential relationships. Heatmaps serve as an effective tool to display these correlations, guiding further investigative efforts.​\n",
        "\n",
        "5.   Hierarchical Analysis of Districts:\n",
        "\n",
        "Top Districts Identification: Sorting and visualizing districts based on the total number of missing persons, as well as by gender-specific counts, helps pinpoint districts with the highest incidences. This hierarchical analysis is crucial for prioritizing resource allocation and intervention strategies."
      ],
      "metadata": {
        "id": "vs9B47Lvb7c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Gender Distribution\n",
        "gender_cols = ['Total_Male', 'Total_Female', 'Total_Transgender']\n",
        "gender_totals = all_districts[gender_cols].sum()\n",
        "\n",
        "# Print exact numbers\n",
        "print(\"Total Missing Persons by Gender (India, 2022):\")\n",
        "print(gender_totals)\n",
        "\n",
        "# Plot with value labels and unique color palette\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x=gender_totals.index, y=gender_totals.values)\n",
        "\n",
        "# Add value labels on top of each bar\n",
        "for i, value in enumerate(gender_totals.values):\n",
        "    ax.text(i, value + max(gender_totals.values)*0.01, f'{int(value):,}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.title(\"Total Missing Persons by Gender (India, 2022)\", fontsize=14)\n",
        "plt.ylabel(\"Number of Cases\")\n",
        "plt.xlabel(\"Gender\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S6bndUDHaU3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping age buckets for comparison\n",
        "age_groups = {\n",
        "    \"Below 12\": ['Male_Below_12_years', 'Female_Below_12_years', 'Transgender_Below_12_years'],\n",
        "    \"12-16\": ['Male_12 years_&_Above_Below_16_years', 'Female_12_years_&_Above_Below_16_years', 'Transgender_12_years_&_Above_Below_16_years'],\n",
        "    \"16-18\": ['Male_16 years_&_Above_Below_18_years', 'Female_16 years_&_Above_Below_18_years', 'Transgender_16 years_&_Above_Below_18_years'],\n",
        "    \"18+\": ['Male_18 years_&_Above', 'Female_18 years_&_Above', 'Transgender_18 years_&_Above']\n",
        "}\n",
        "\n",
        "age_df = pd.DataFrame({age: df[cols].sum().values for age, cols in age_groups.items()},)\n",
        "age_df_gender_wise = age_df.T  # Now rows = gender, columns = age groups\n",
        "\n",
        "# Plot\n",
        "ax = age_df_gender_wise.plot(kind='bar', stacked=True, figsize=(10, 6), colormap=\"Accent\")\n",
        "\n",
        "# Title and labels\n",
        "plt.title(\"Gender-wise and Age-wise Distribution of Missing Persons\")\n",
        "plt.ylabel(\"Number of Cases\")\n",
        "plt.xlabel(\"Gender\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title=\"Gender\")\n",
        "\n",
        "# Add total numeric labels on top of each bar\n",
        "totals = age_df_gender_wise.sum(axis=1)\n",
        "for i, total in enumerate(totals):\n",
        "    ax.text(i, total + max(totals)*0.01, f'{int(total):,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R3GievmjdXvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_summary = df.groupby(\"Region\")[gender_cols].sum()\n",
        "\n",
        "region_summary.plot(kind=\"bar\", figsize=(12,6), stacked=True, colormap='Pastel1')\n",
        "plt.title(\"Region-wise Gender Distribution of Missing Persons\")\n",
        "plt.ylabel(\"Number of Cases\")\n",
        "plt.xlabel(\"Region\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jbsLxOFIdvcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only age-wise numeric columns and Grand_Total\n",
        "age_cols = [\n",
        "    'Male_Below_12_years', 'Female_Below_12_years', 'Transgender_Below_12_years',\n",
        "    'Male_12 years_&_Above_Below_16_years', 'Female_12_years_&_Above_Below_16_years', 'Transgender_12_years_&_Above_Below_16_years',\n",
        "    'Male_16 years_&_Above_Below_18_years', 'Female_16 years_&_Above_Below_18_years', 'Transgender_16 years_&_Above_Below_18_years',\n",
        "    'Male_18 years_&_Above', 'Female_18 years_&_Above', 'Transgender_18 years_&_Above',\n",
        "    'Grand_Total'\n",
        "]\n",
        "\n",
        "# Compute correlation matrix\n",
        "age_corr = all_districts[age_cols].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(age_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title(\"Correlation Heatmap: Age Categories vs Total Missing Cases\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8to88IC9aMme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate region-wise total missing persons\n",
        "region_totals = all_districts.groupby('Region')['Grand_Total'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Print the numeric values\n",
        "print(\"Total Missing Persons by Region (India, 2022):\\n\")\n",
        "print(region_totals)\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "colors = sns.color_palette('Set3', len(region_totals))\n",
        "plt.pie(region_totals, labels=region_totals.index, autopct='%1.1f%%', startangle=140, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
        "plt.title(\"Regional Distribution of Missing Persons (India, 2022)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S7udtvMBajoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort and get top 10 districts by Grand_Total, Total_Male, and Total_Female\n",
        "top_grand_total = all_districts.sort_values('Grand_Total', ascending=False).head(10)\n",
        "top_male_total = all_districts.sort_values('Total_Male', ascending=False).head(10)\n",
        "top_female_total = all_districts.sort_values('Total_Female', ascending=False).head(10)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "# Plot 1: Grand Total\n",
        "plt.subplot(3, 1, 1)\n",
        "sns.barplot(x='Grand_Total', y='District', data=top_grand_total, palette=\"Reds_r\")\n",
        "plt.title(\"Top 10 Districts by Total Missing Persons\")\n",
        "plt.xlabel(\"Total Missing Persons\")\n",
        "plt.ylabel(\"District\")\n",
        "\n",
        "# Plot 2: Total Male\n",
        "plt.subplot(3, 1, 2)\n",
        "sns.barplot(x='Total_Male', y='District', data=top_male_total, palette=\"Blues_r\")\n",
        "plt.title(\"Top 10 Districts by Missing Males\")\n",
        "plt.xlabel(\"Total Missing Males\")\n",
        "plt.ylabel(\"District\")\n",
        "\n",
        "# Plot 3: Total Female\n",
        "plt.subplot(3, 1, 3)\n",
        "sns.barplot(x='Total_Female', y='District', data=top_female_total, palette=\"Purples_r\")\n",
        "plt.title(\"Top 10 Districts by Missing Females\")\n",
        "plt.xlabel(\"Total Missing Females\")\n",
        "plt.ylabel(\"District\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EVx4Yp--aXks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution Plots for Features"
      ],
      "metadata": {
        "id": "k9PFi2rGjU34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected numeric columns for EDA (from scaled DataFrame)\n",
        "numeric_cols_total = total_districts.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Calculate subplot layout (4 columns per row)\n",
        "n_cols = 4\n",
        "n_total = len(numeric_cols_total)\n",
        "n_rows = math.ceil(n_total / n_cols)\n",
        "\n",
        "# Create the subplots\n",
        "plt.figure(figsize=(20, n_rows * 4))\n",
        "\n",
        "for i, col in enumerate(numeric_cols_total):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    sns.histplot(data=total_districts, x=col, kde=True, bins=20, color='skyblue')\n",
        "    plt.title(f'Distribution of {col}', fontsize=10)\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('')\n",
        "    plt.grid(True)\n",
        "\n",
        "# Title and layout adjustments\n",
        "plt.suptitle(\"Distribution of Features (Total Districts)\", fontsize=18, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wDcerZbeihC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Scaling the Data"
      ],
      "metadata": {
        "id": "qQNyl8vq24ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the datasets scaling, we had the following methodology:\n",
        "1.   Feature and Target Separation:\n",
        "\n",
        "Extract features by removing the 'Region' column from the dataset.​ Assign the 'Region' column as the target variable.​\n",
        "\n",
        "2.    Feature Selection with SelectKBest:\n",
        "Utilize the SelectKBest method with the f_classif scoring function to identify the top 20 features that have the strongest relationship with the target variable.​\n",
        "\n",
        "Transform the dataset to retain only these selected features.​\n",
        "\n",
        "3.   Data Scaling Using RobustScaler:\n",
        "\n",
        "Initialize the RobustScaler, which scales features using statistics that are robust to outliers by removing the median and scaling according to the interquartile range. ​\n"
      ],
      "metadata": {
        "id": "jEMx42wMXsGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Separate target and features\n",
        "X_raw = total_districts.drop(columns=['Region'])\n",
        "y = total_districts['Region']\n",
        "\n",
        "# Apply SelectKBest to get top N features\n",
        "selector = SelectKBest(score_func=f_classif, k=20)\n",
        "X_selected = selector.fit_transform(X_raw.select_dtypes(include=['int64', 'float64']), y)\n",
        "\n",
        "# Create new DataFrame with selected columns\n",
        "selected_columns = X_raw.select_dtypes(include=['int64', 'float64']).columns[selector.get_support()]\n",
        "total_districts = total_districts[selected_columns.tolist() + ['Region']]\n"
      ],
      "metadata": {
        "id": "U5FMaZNpgJLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = RobustScaler()\n",
        "\n",
        "# Select numeric columns for scaling\n",
        "numeric_cols_total = total_districts.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create a copy and scale the data\n",
        "total_districts_scaled = total_districts.copy()\n",
        "total_districts_scaled[numeric_cols_total] = scaler.fit_transform(total_districts_scaled[numeric_cols_total])\n",
        "\n"
      ],
      "metadata": {
        "id": "ybWEMQKA2b36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Splitting Data into Train and Test Sets"
      ],
      "metadata": {
        "id": "WI3YHIJQ3ioa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the test_train_split, we applied the follwoing:\n",
        "\n",
        "\n",
        "Data Consolidation:\n",
        "\n",
        "Combine scaled features with the 'Region' target variable into a single DataFrame.​\n",
        "\n",
        "Filtering Rare Classes:\n",
        "\n",
        "Remove classes in 'Region' with fewer than three samples to ensure sufficient data for analysis.​\n",
        "\n",
        "Feature and Target Separation:\n",
        "\n",
        "Assign numeric feature columns to X and the 'Region' column to y.​\n",
        "\n",
        "Data Splitting with Stratification:\n",
        "\n",
        "Split data into training and testing sets (80-20 split) while preserving class distribution using the stratify=y parameter. ​\n",
        "\n",
        "Data Export:\n",
        "\n",
        "Save the training set as \"train_data.csv\" and the testing set as \"test_data.csv\"."
      ],
      "metadata": {
        "id": "YniXFDdYsU26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge features and target into a single DataFrame for consistency\n",
        "data = total_districts_scaled.copy()\n",
        "data['Region'] = total_districts['Region']\n",
        "\n",
        "# Filter out classes with fewer than 3 samples before splitting\n",
        "class_counts = data['Region'].value_counts()\n",
        "valid_classes = class_counts[class_counts >= 3].index\n",
        "data = data[data['Region'].isin(valid_classes)]\n",
        "\n",
        "\n",
        "if 'Region' in numeric_cols_total:\n",
        "    numeric_cols_total.remove('Region')\n",
        "\n",
        "X = data[numeric_cols_total]\n",
        "y = data['Region']\n",
        "\n",
        "# Use stratify parameter to maintain class distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "test_data = pd.concat([X_test, y_test], axis=1)\n",
        "train_data.to_csv(\"train_data.csv\", index=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ZM5H_gcZ3epf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Remove Underrepresented Classes\n",
        "Classes with less than 3 samples cause issues in both training and evaluation."
      ],
      "metadata": {
        "id": "wcNipr2Y-Vj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop classes with fewer than 3 samples\n",
        "value_counts = y_train.value_counts()\n",
        "valid_classes = value_counts[value_counts >= 5].index\n",
        "\n",
        "X_train_filtered = X_train[y_train.isin(valid_classes)]\n",
        "y_train_filtered = y_train[y_train.isin(valid_classes)]\n",
        "X_test_filtered = X_test[y_test.isin(valid_classes)]\n",
        "y_test_filtered = y_test[y_test.isin(valid_classes)]\n"
      ],
      "metadata": {
        "id": "9DrRUmJb-QI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.PCA - Principal Component Analysis"
      ],
      "metadata": {
        "id": "xanM_ph53pdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA without limiting components\n",
        "pca_full = PCA()\n",
        "X_train_pca_full = pca_full.fit_transform(X_train)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca_full.explained_variance_ratio_), marker='o', linestyle='--', color='darkblue')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by PCA Components')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.9, color='red', linestyle='--', label='90% Variance')\n",
        "plt.axhline(y=0.95, color='green', linestyle='--', label='95% Variance')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ocVvU5wu3mBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA\n",
        "pca = PCA(n_components=5)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Check explained variance\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Total Explained Variance (5 components):\", pca.explained_variance_ratio_.sum())\n"
      ],
      "metadata": {
        "id": "4bxsBWo83-2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  9.LDA – Linear Discriminant Analysis"
      ],
      "metadata": {
        "id": "2pzAmhaR4VMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LDA(n_components=2)\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "X_test_lda = lda.transform(X_test)\n",
        "\n",
        "# Pad with zeros to make it 2D for plotting later\n",
        "X_train_lda = np.hstack([X_train_lda, np.zeros_like(X_train_lda)])\n",
        "X_test_lda = np.hstack([X_test_lda, np.zeros_like(X_test_lda)])\n"
      ],
      "metadata": {
        "id": "ST9yeBMl4GXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_full = LDA(n_components=None)\n",
        "lda_full.fit(X_train, y_train)\n",
        "print(\"LDA Explained Variance Ratio:\", lda_full.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "EUyUB8Ag4sSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.SVD"
      ],
      "metadata": {
        "id": "G1KifXZW5JNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying more components initially to observe explained variance\n",
        "svd_check = TruncatedSVD(n_components=5, random_state=42)\n",
        "svd_check.fit(X_train)\n",
        "\n",
        "explained_variance_svd = svd_check.explained_variance_ratio_.cumsum()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance_svd) + 1), explained_variance_svd, marker='o', linestyle='--', color='orange')\n",
        "plt.title('SVD - Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.90, color='red', linestyle='--', label='90% Threshold')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9HD3Pv5E5Mzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final SVD for visualization\n",
        "svd = TruncatedSVD(n_components=5, random_state=42)\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)\n"
      ],
      "metadata": {
        "id": "lf4_Dnpe5ewS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.MDS"
      ],
      "metadata": {
        "id": "unkJYBoM5kRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the training and testing data\n",
        "X_combined = np.concatenate((X_train, X_test), axis=0)\n",
        "\n",
        "# Fit MDS on the combined data to ensure a common embedding space\n",
        "mds = MDS(n_components=2, random_state=42, n_init=1, max_iter=300, dissimilarity='euclidean')\n",
        "X_combined_mds = mds.fit_transform(X_combined)\n",
        "\n",
        "# Split the combined MDS output back into train and test sets\n",
        "X_train_mds = X_combined_mds[:len(X_train)]\n",
        "X_test_mds = X_combined_mds[len(X_train):]\n",
        "\n",
        "# Reset y_train and y_test indices to match transformed arrays\n",
        "y_train_mds = y_train.reset_index(drop=True)\n",
        "y_test_mds = y_test.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "HN-9dlAx5f2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.T-SNE"
      ],
      "metadata": {
        "id": "yFy_iRB-5tYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine training and testing data\n",
        "X_combined = np.concatenate((X_train, X_test), axis=0)\n",
        "n_samples = X_combined.shape[0]\n",
        "\n",
        "for perplexity_val in [5, 10, 20, 30]:\n",
        "    tsne = TSNE(n_components=3, random_state=42, perplexity=n_samples-1, max_iter=2000)\n",
        "    X_combined_tsne = tsne.fit_transform(X_combined)\n",
        "    # Evaluate the classifier accuracy after splitting train/test\n",
        "\n",
        "# Split back into train and test sets\n",
        "X_train_tsne = X_combined_tsne[:len(X_train)]\n",
        "X_test_tsne = X_combined_tsne[len(X_train):]\n",
        "\n",
        "# Reset indices for labels\n",
        "y_train_tsne = y_train.reset_index(drop=True)\n",
        "y_test_tsne = y_test.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "J4bpg_qI6XeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.UMap\n"
      ],
      "metadata": {
        "id": "8btCUlmU6Zap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)\n",
        "X_train_umap = umap_model.fit_transform(X_train)\n",
        "X_test_umap = umap_model.transform(X_test)\n"
      ],
      "metadata": {
        "id": "dNeONBdd6hr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.1 Clustering on UMap"
      ],
      "metadata": {
        "id": "a-uoH_RU6zkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inertia = []\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_train_umap)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    score = silhouette_score(X_train_umap, kmeans.labels_)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot Elbow & Silhouette\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(K_range, inertia, 'o-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(K_range, silhouette_scores, 's-', color='green')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score vs. k')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "njJ1QK3F6rug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elbow Method: The inertia sharply drops and begins to level off around k = 3 to 4, indicating that adding more clusters beyond this point yields diminishing returns — suggesting 3 or 4 clusters may be optimal.\n",
        "\n",
        "Silhouette Score: The highest silhouette score occurs at k = 2, indicating the best-defined clusters at that value. However, the score drops significantly after k = 3, supporting the idea that 2–3 clusters produce more coherent and well-separated groups.\n"
      ],
      "metadata": {
        "id": "f7XGKIrLsqeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "umap_clusters = kmeans.fit_predict(X_train_umap)\n",
        "\n",
        "# Clustering visualization\n",
        "umap_cluster_df = pd.DataFrame(X_train_umap, columns=['UMAP1', 'UMAP2'])\n",
        "umap_cluster_df['Cluster'] = umap_clusters\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=umap_cluster_df, x='UMAP1', y='UMAP2', hue='Cluster', palette='Set2', s=60)\n",
        "plt.title(\"KMeans Clustering (k=3) on UMAP Output\")\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uqC2TpDj4b_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_umap, y_train)\n",
        "y_pred = knn.predict(X_test_umap)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"UMAP + KNN Classification Accuracy:\", round(acc, 4))"
      ],
      "metadata": {
        "id": "yRzXz8zu5-aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Comparative Visualization of Dimensionality Reduction Methods"
      ],
      "metadata": {
        "id": "YFodRQnf89Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create label-to-color mapping\n",
        "unique_labels = sorted(y_train.unique())\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "colors = sns.color_palette(\"tab10\", len(unique_labels))\n",
        "color_map = {label: colors[idx] for label, idx in label_mapping.items()}\n",
        "legend_handles = [mpatches.Patch(color=color_map[label], label=label) for label in unique_labels]\n",
        "\n",
        "# Data to plot\n",
        "method_names = [\"PCA\", \"SVD\", \"t-SNE\", \"MDS\", \"UMAP\"]\n",
        "method_data = [X_train_pca, X_train_svd, X_train_tsne, X_train_mds, X_train_umap]\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 10), constrained_layout=True)\n",
        "fig.suptitle(\"Visualization of Dimensionality Reduction Methods\", fontsize=20, fontweight=\"bold\", color=\"darkred\")\n",
        "\n",
        "for ax, data, name in zip(axes.flat, method_data, method_names):\n",
        "    for label in unique_labels:\n",
        "        idxs = y_train == label\n",
        "        ax.scatter(\n",
        "            data[idxs, 0], data[idxs, 1],\n",
        "            color=color_map[label], label=label, edgecolors='white', linewidth=0.5, s=40, alpha=0.9\n",
        "        )\n",
        "    ax.set_title(name, fontsize=16, fontweight=\"bold\")\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    ax.set_xlabel(\"Component 1\")\n",
        "    ax.set_ylabel(\"Component 2\")\n",
        "\n",
        "# Turn off extra subplot if any\n",
        "if len(method_data) < len(axes.flat):\n",
        "    axes.flat[-1].axis('off')\n",
        "\n",
        "# Add legend\n",
        "fig.legend(handles=legend_handles, loc='lower right', bbox_to_anchor=(0.93, 0.93), fontsize=12, title=\"Regions\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "3iXWUvia9APN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15.Model Evaluation"
      ],
      "metadata": {
        "id": "f8xaYGyZ9N4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tune_knn(X_train_red, y_train_red, X_test_red, y_test_red, method=\"KNN\"):\n",
        "    param_grid = {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'p': [1, 2]  # Manhattan and Euclidean distances\n",
        "    }\n",
        "\n",
        "    model = KNeighborsClassifier()\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train_red, y_train_red)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test_red)\n",
        "\n",
        "    acc = accuracy_score(y_test_red, y_pred)\n",
        "    class_report = classification_report(y_test_red, y_pred, zero_division=1)\n",
        "    conf_matrix = confusion_matrix(y_test_red, y_pred)\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    print(f\"{method} Tuned Accuracy: {acc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"classification_report\": class_report,\n",
        "        \"confusion_matrix\": conf_matrix,\n",
        "        \"best_params\": best_params\n",
        "    }\n"
      ],
      "metadata": {
        "id": "JNZe0a9s9M79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_knn = {}\n",
        "results_knn['PCA'] = tune_knn(X_train_pca, y_train, X_test_pca, y_test, method=\"PCA + KNN\")\n",
        "results_knn['LDA'] = tune_knn(X_train_lda, y_train, X_test_lda, y_test, method=\"LDA + KNN\")\n",
        "results_knn['SVD'] = tune_knn(X_train_svd, y_train, X_test_svd, y_test, method=\"SVD + KNN\")\n",
        "results_knn['MDS'] = tune_knn(X_train_mds, y_train_mds, X_test_mds, y_test_mds, method=\"MDS + KNN\")\n",
        "results_knn['t-SNE'] = tune_knn(X_train_tsne, y_train_tsne, X_test_tsne, y_test_tsne, method=\"t-SNE + KNN\")\n"
      ],
      "metadata": {
        "id": "PXgavtpn9aMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = {method: results_knn[\"accuracy\"] for method, results_knn in results_knn.items()}\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(accuracies.keys(), accuracies.values(), color='skyblue')\n",
        "plt.xlabel(\"Dimensionality Reduction Method\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QWD7gszl9xxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Accuracy Across Dimensionality Reduction Techniques\n",
        "This chart presents the classification accuracy achieved after applying various dimensionality reduction methods prior to model training.\n",
        "\n",
        "Observations:\n",
        "\n",
        "> LDA (Linear Discriminant Analysis) achieved the highest accuracy (~75%), suggesting it retained the most class-relevant information.\n",
        "\n",
        "> PCA and SVD performed moderately (~50%), indicating partial retention of informative features.\n",
        "\n",
        "> MDS and t-SNE showed the lowest accuracy (~37%), reflecting limited utility for predictive modeling.\n",
        "\n",
        "Analytical Interpretation:\n",
        "\n",
        "    Supervised vs. Unsupervised:\n",
        "LDA, being a supervised technique, leverages class labels to maximize inter-class separation. In contrast, PCA, SVD, MDS, and t-SNE are unsupervised, optimizing for variance or distance preservation without regard to class structure.\n",
        "\n",
        "    Optimization Objectives:\n",
        "PCA/SVD prioritize global variance, which may not align with class boundaries.\n",
        "\n",
        "MDS/t-SNE preserve local neighborhood structure, making them more suitable for visualization than classification.\n",
        "\n",
        "    Dimensionality Trade-off:\n",
        "Unsupervised methods may compress features critical for class discrimination, leading to reduced model performance."
      ],
      "metadata": {
        "id": "RNnhtAhAs8cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"Best Params: {result['best_params']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "3VcYdW7h9y8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Classification Report:\\n{result['classification_report']}\\n\")\n"
      ],
      "metadata": {
        "id": "8XK4R2KW_UFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These differences reflect challenges inherent to a dataset that's fundamentally classification based:\n",
        "\n",
        "    Small Sample Size & Imbalance:\n",
        "With only 8 samples across four classes, the metrics become extremely sensitive to single-sample misclassifications, which can skew precision, recall, and f1-scores. This small, imbalanced dataset makes it difficult for any algorithm to generalize effectively.\n",
        "\n",
        "    Supervised vs. Unsupervised Dimensionality Reduction:\n",
        "Techniques like PCA, SVD, MDS, and t-SNE are unsupervised and focus solely on data variance or distance preservation, often disregarding class labels. As a result, they might not capture the class-specific features crucial for this classification task.\n",
        "\n",
        "In contrast, LDA, which is supervised, uses class information to maximize inter-class separability, hence its better performance.\n",
        "\n",
        "    Information Loss During Reduction:\n",
        "For a classification-based dataset, preserving discriminative features is essential. Unsupervised methods may inadvertently discard or dilute these features during the dimensionality reduction process, leading to lower classification accuracy in the reduced space."
      ],
      "metadata": {
        "id": "jsiAX8Q9t4Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Confusion Matrix:\\n{result['confusion_matrix']}\\n\")\n"
      ],
      "metadata": {
        "id": "GKxEV6Wy_X1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method, result in results_knn.items():\n",
        "    print(f\"{method} Confusion Matrix:\\n{result['confusion_matrix']}\\n\")\n"
      ],
      "metadata": {
        "id": "0yHblFF9_cHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion,\n",
        "1.    The primary issue lies in applying unsupervised dimensionality reduction techniques to a dataset that is inherently classification-based, where preserving class-discriminative features is crucial.\n",
        "\n",
        "2.    Methods like PCA, SVD, MDS, and t-SNE optimize for variance or local structure without considering class labels, leading to significant information loss and poor classification performance.\n",
        "\n",
        "\n",
        "3.    Additionally, working with a small and imbalanced dataset amplifies these challenges, making results highly sensitive to individual misclassifications.\n",
        "\n",
        "\n",
        "4.   To improve, we should prioritize supervised techniques like LDA when the goal is classification, ensure adequate and balanced class representation, and validate results using larger datasets or cross-validation to reduce variance in performance and improve model robustness."
      ],
      "metadata": {
        "id": "ZwFngzK6uauX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3StaXRrzuaRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpzGPFixZj6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
